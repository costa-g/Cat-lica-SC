# scripts/análise_eleitoral.py

import pandas as pd
from glob import glob
import fitz
import nltk
from nltk.corpus import stopwords
from collections import Counter
import os
import folium

# Função para carregar todos os arquivos em uma pasta específica
def load_data_from_folder(folder_path, file_pattern="*.csv"):
    all_files = glob(os.path.join(folder_path, file_pattern))
    df_list = [pd.read_csv(file, sep=';', encoding='latin1') for file in all_files]
    return pd.concat(df_list, ignore_index=True)

# Caminhos para as pastas de dados
data_paths = {
    "candidatos": "./data/candidatos/",
    "candidatos_bens": "./data/candidatos_bens/",
    "candidatos_info_complementar": "./data/candidatos_info_complementar/",
    "candidatos_redes_sociais": "./data/candidatos_redes_sociais/",
    "coligacoes": "./data/coligacoes/",
    "motivo_cassacao": "./data/motivo_cassacao/",
    "vagas": "./data/vagas/"
}

# Carregando os dados
dados_candidatos = load_data_from_folder(data_paths["candidatos"])
dados_bens = load_data_from_folder(data_paths["candidatos_bens"])
dados_info_complementar = load_data_from_folder(data_paths["candidatos_info_complementar"])
dados_redes_sociais = load_data_from_folder(data_paths["candidatos_redes_sociais"])
dados_coligacoes = load_data_from_folder(data_paths["coligacoes"])
dados_motivo_cassacao = load_data_from_folder(data_paths["motivo_cassacao"])
dados_vagas = load_data_from_folder(data_paths["vagas"])

# Exibir uma amostra de cada DataFrame para verificar o carregamento
print("Candidatos:", dados_candidatos.head())
print("Bens:", dados_bens.head())
print("Informações Complementares:", dados_info_complementar.head())
print("Redes Sociais:", dados_redes_sociais.head())
print("Coligações:", dados_coligacoes.head())
print("Motivos de Cassação:", dados_motivo_cassacao.head())
print("Vagas:", dados_vagas.head())

# Verificando a quantidade de valores nulos em cada DataFrame
print("Valores nulos em cada conjunto de dados:")
print("Candidatos:", dados_candidatos.isnull().sum())
print("Bens:", dados_bens.isnull().sum())
print("Informações Complementares:", dados_info_complementar.isnull().sum())
print("Redes Sociais:", dados_redes_sociais.isnull().sum())
print("Coligações:", dados_coligacoes.isnull().sum())
print("Motivos de Cassação:", dados_motivo_cassacao.isnull().sum())
print("Vagas:", dados_vagas.isnull().sum())

# Exemplo de tratamento: preencher nulos com valor padrão ou remover colunas com muitos nulos
# Aqui, substituímos nulos em colunas de redes sociais por 'N/A' como exemplo
dados_redes_sociais.fillna("N/A", inplace=True)

# Para colunas numéricas de bens, preenchendo com zero
dados_bens['VR_BEM_CANDIDATO'] = dados_bens['VR_BEM_CANDIDATO'].fillna(0).astype(str).str.replace(',', '.').astype(float)

# Convertendo colunas de data para o tipo datetime com dayfirst=True para formato dia/mês/ano
dados_candidatos['DT_NASCIMENTO'] = pd.to_datetime(dados_candidatos['DT_NASCIMENTO'], dayfirst=True, errors='coerce')
dados_candidatos['DT_ELEICAO'] = pd.to_datetime(dados_candidatos['DT_ELEICAO'], dayfirst=True, errors='coerce')
dados_vagas['DT_ELEICAO'] = pd.to_datetime(dados_vagas['DT_ELEICAO'], dayfirst=True, errors='coerce')
dados_vagas['DT_POSSE'] = pd.to_datetime(dados_vagas['DT_POSSE'], dayfirst=True, errors='coerce')

# Padronizando o texto em letras minúsculas para colunas selecionadas
dados_candidatos['SG_PARTIDO'] = dados_candidatos['SG_PARTIDO'].str.lower()
dados_candidatos['DS_CARGO'] = dados_candidatos['DS_CARGO'].str.lower()
dados_candidatos['SG_UF'] = dados_candidatos['SG_UF'].str.upper()  # UF em maiúsculas para consistência
dados_coligacoes['NM_PARTIDO'] = dados_coligacoes['NM_PARTIDO'].str.lower()
dados_coligacoes['SG_UF'] = dados_coligacoes['SG_UF'].str.upper()


# scripts/análise_eleitoral.py

# Filtrar candidatos para o cargo de prefeito e eleitos
prefeitos = dados_candidatos[dados_candidatos['DS_CARGO'] == 'prefeito']
prefeitos_eleitos = prefeitos[prefeitos['DS_SIT_TOT_TURNO'] == 'eleito']

# Somar os bens declarados para candidatos eleitos e não eleitos
# bens_eleitos = dados_bens[dados_bens['SQ_CANDIDATO'].isin(prefeitos_eleitos['SQ_CANDIDATO'])]
# soma_bens_eleitos = bens_eleitos.groupby('SQ_CANDIDATO')['VR_BEM_CANDIDATO'].sum().mean()

# bens_nao_eleitos = dados_bens[~dados_bens['SQ_CANDIDATO'].isin(prefeitos_eleitos['SQ_CANDIDATO'])]
# soma_bens_nao_eleitos = bens_nao_eleitos.groupby('SQ_CANDIDATO')['VR_BEM_CANDIDATO'].sum().mean()

# Recalculando a média de bens para eleitos e não eleitos
bens_eleitos = dados_bens[dados_bens['SQ_CANDIDATO'].isin(prefeitos_eleitos['SQ_CANDIDATO'])]
soma_bens_eleitos = bens_eleitos['VR_BEM_CANDIDATO'].mean()  # Média de bens dos eleitos

bens_nao_eleitos = dados_bens[~dados_bens['SQ_CANDIDATO'].isin(prefeitos_eleitos['SQ_CANDIDATO'])]
soma_bens_nao_eleitos = bens_nao_eleitos['VR_BEM_CANDIDATO'].mean()  # Média de bens dos não eleitos

# Exibir resultado
print(f"Média de bens declarados pelos prefeitos eleitos: {soma_bens_eleitos}")
print(f"Média de bens declarados pelos prefeitos não eleitos: {soma_bens_nao_eleitos}")

# scripts/análise_eleitoral.py

# Contar o número de partidos por coligação
dados_coligacoes['NUMERO_PARTIDOS'] = dados_coligacoes['DS_COMPOSICAO_FEDERACAO'].str.count(',') + 1

# Agrupar por coligação e verificar o número de candidatos eleitos por tamanho de coligação
coligacoes_eleitos = dados_candidatos[dados_candidatos['DS_SIT_TOT_TURNO'] == 'eleito']
coligacoes_resultados = coligacoes_eleitos.groupby('SQ_COLIGACAO').size().reset_index(name='NUM_ELEITOS')
coligacoes_detalhadas = dados_coligacoes.merge(coligacoes_resultados, on='SQ_COLIGACAO')

# Exibir as coligações com maior número de eleitos e de partidos
print(coligacoes_detalhadas[['NM_COLIGACAO', 'NUMERO_PARTIDOS', 'NUM_ELEITOS']].sort_values(by='NUM_ELEITOS', ascending=False))

# scripts/análise_eleitoral.py

# Contar candidatos por partido e UF
partidos_por_uf = dados_candidatos.groupby(['SG_UF', 'SG_PARTIDO']).size().reset_index(name='NUM_CANDIDATOS')
maior_partido_por_uf = partidos_por_uf.loc[partidos_por_uf.groupby('SG_UF')['NUM_CANDIDATOS'].idxmax()]

# Exibir resultado
print("Partido com maior quantidade de candidatos por UF:")
print(maior_partido_por_uf)

# scripts/análise_eleitoral.py

# Mapeamento de UFs para regiões
ufs_para_regioes = {
    # Norte
    "AC": "Norte", "AP": "Norte", "AM": "Norte", "PA": "Norte", "RO": "Norte", "RR": "Norte", "TO": "Norte",
    # Nordeste
    "AL": "Nordeste", "BA": "Nordeste", "CE": "Nordeste", "MA": "Nordeste", "PB": "Nordeste",
    "PE": "Nordeste", "PI": "Nordeste", "RN": "Nordeste", "SE": "Nordeste",
    # Centro-Oeste
    "DF": "Centro-Oeste", "GO": "Centro-Oeste", "MT": "Centro-Oeste", "MS": "Centro-Oeste",
    # Sudeste
    "ES": "Sudeste", "MG": "Sudeste", "RJ": "Sudeste", "SP": "Sudeste",
    # Sul
    "PR": "Sul", "RS": "Sul", "SC": "Sul"
}

# Adicionar coluna de região
dados_candidatos['REGIAO'] = dados_candidatos['SG_UF'].map(ufs_para_regioes)

# Contagem de candidatos por partido e região
candidatos_por_regiao = dados_candidatos.groupby(['REGIAO', 'SG_PARTIDO']).size().reset_index(name='NUM_CANDIDATOS')

# Exibir resultado
print("Distribuição de candidaturas por partido e região:")
print(candidatos_por_regiao)

# scripts/análise_eleitoral.py

# Filtrar para os cargos de prefeito, vice-prefeito e vereador
cargos_importantes = ['prefeito', 'vice-prefeito', 'vereador']
dados_cargos = dados_candidatos[dados_candidatos['DS_CARGO'].isin(cargos_importantes)]

# Contar candidatos por partido e UF
partido_dominante_uf = dados_cargos.groupby(['SG_UF', 'SG_PARTIDO']).size().reset_index(name='TOTAL_CANDIDATOS')
partido_dominante_uf = partido_dominante_uf.loc[partido_dominante_uf.groupby('SG_UF')['TOTAL_CANDIDATOS'].idxmax()]

# Exibir resultado
print("Partido com maior número de candidatos em cada UF (Prefeito, Vice e Vereadores):")
print(partido_dominante_uf)

# scripts/análise_eleitoral.py

# Filtrar candidatos indígenas e quilombolas com merge para trazer SG_UF
candidatos_indigenas = dados_info_complementar[dados_info_complementar['CD_ETNIA_INDIGENA'] != 0]
candidatos_quilombolas = dados_info_complementar[dados_info_complementar['ST_QUILOMBOLA'] == 'S']

# Adicionar SG_UF aos dados de indígenas e quilombolas a partir dos dados de candidatos principais
candidatos_indigenas = candidatos_indigenas.merge(dados_candidatos[['SQ_CANDIDATO', 'SG_UF']], on='SQ_CANDIDATO', how='left')
candidatos_quilombolas = candidatos_quilombolas.merge(dados_candidatos[['SQ_CANDIDATO', 'SG_UF']], on='SQ_CANDIDATO', how='left')

# Adicionar coluna de região para análise
candidatos_indigenas['REGIAO'] = candidatos_indigenas['SG_UF'].map(ufs_para_regioes)
candidatos_quilombolas['REGIAO'] = candidatos_quilombolas['SG_UF'].map(ufs_para_regioes)

# Contar candidatos indígenas e quilombolas por região
indigenas_por_regiao = candidatos_indigenas.groupby('REGIAO').size().reset_index(name='NUM_INDIGENAS')
quilombolas_por_regiao = candidatos_quilombolas.groupby('REGIAO').size().reset_index(name='NUM_QUILOMBOLAS')

# Exibir resultados
print("Distribuição de candidatos indígenas por região:")
print(indigenas_por_regiao)
print("\nDistribuição de candidatos quilombolas por região:")
print(quilombolas_por_regiao)

# scripts/análise_eleitoral.py

# Extraindo a categoria de rede social a partir do URL
dados_redes_sociais['TIPO_REDE'] = dados_redes_sociais['DS_URL'].str.extract(r'(facebook|instagram|twitter|youtube|linkedin)', expand=False).fillna('outros')

# Contagem de redes sociais por partido e UF
redes_por_partido_uf = dados_redes_sociais.groupby(['SG_UF', 'TIPO_REDE']).size().reset_index(name='NUM_CANDIDATOS')

# Exibir resultado
print("Rede social preferida dos candidatos por partido e UF:")
print(redes_por_partido_uf)

# scripts/análise_eleitoral.py

# Baixar stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('portuguese'))

# Função para extrair texto dos PDFs
def extract_text_from_pdf(file_path):
    text = ""
    with fitz.open(file_path) as pdf:
        for page_num in range(pdf.page_count):
            text += pdf[page_num].get_text()
    return text

# Caminho para as propostas
path_propostas = './data/candidatos_propostas_governo/SC/'

# Processar cada PDF e extrair palavras-chave
all_terms = []
for file_name in os.listdir(path_propostas):
    if file_name.endswith(".pdf"):
        file_path = os.path.join(path_propostas, file_name)
        text = extract_text_from_pdf(file_path)
        # Tokenizar e filtrar stopwords
        tokens = [word for word in text.lower().split() if word.isalpha() and word not in stop_words]
        all_terms.extend(tokens)

# Contagem das palavras mais frequentes
termos_frequentes = Counter(all_terms).most_common(10)

# Exibir termos mais frequentes
print("Principais termos nas propostas de governo:")
print(termos_frequentes)

# scripts/análise_eleitoral.py

# Suponha que você tenha uma tabela com os resultados por município e partido vencedor
# Vamos criar uma amostra de dados fictícios para demonstrar

# Dados fictícios para o exemplo
resultados_municipios = pd.DataFrame({
    'MUNICIPIO': ['Município A', 'Município B', 'Município C'],
    'LATITUDE': [-23.5505, -22.9068, -21.1775],
    'LONGITUDE': [-46.6333, -43.1729, -44.8709],
    'PARTIDO_VENCEDOR': ['partido_a', 'partido_b', 'partido_c']
})

# Mapa inicial centrado no Brasil
mapa_brasil = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)

# Dicionário de cores para cada partido
cores_partidos = {'partido_a': 'blue', 'partido_b': 'red', 'partido_c': 'green'}

# Adicionar marcadores ao mapa para cada município
for _, row in resultados_municipios.iterrows():
    folium.CircleMarker(
        location=(row['LATITUDE'], row['LONGITUDE']),
        radius=6,
        color=cores_partidos.get(row['PARTIDO_VENCEDOR'], 'gray'),
        fill=True,
        fill_opacity=0.7,
        popup=row['MUNICIPIO']
    ).add_to(mapa_brasil)

# Salvar o mapa como um arquivo HTML
mapa_brasil.save("resultado_eleicoes_mapa.html")
print("Mapa gerado e salvo como resultado_eleicoes_mapa.html")

# scripts/análise_eleitoral.py

# Exemplo para salvar os resultados de cada insight em CSV
indigenas_por_regiao.to_csv('output/indigenas_por_regiao.csv', index=False)
quilombolas_por_regiao.to_csv('output/quilombolas_por_regiao.csv', index=False)
maior_partido_por_uf.to_csv('output/maior_partido_por_uf.csv', index=False)
partido_dominante_uf.to_csv('output/partido_dominante_uf.csv', index=False)
candidatos_por_regiao.to_csv('output/distribuicao_partido_regiao.csv', index=False)
coligacoes_detalhadas.to_csv('output/coligacoes_detalhadas.csv', index=False)
redes_por_partido_uf.to_csv('output/redes_por_partido_uf.csv', index=False)
pd.DataFrame(termos_frequentes, columns=['Termo', 'Frequência']).to_csv('output/termos_propostas.csv', index=False)

import matplotlib.pyplot as plt

# Gráfico de barras para comparação de bens declarados
plt.figure(figsize=(8, 5))
plt.bar(['Eleitos', 'Não Eleitos'], [soma_bens_eleitos, soma_bens_nao_eleitos], color=['blue', 'red'])
plt.title("Média de Bens Declarados por Prefeitos Eleitos vs Não Eleitos")
plt.ylabel("Média de Bens Declarados (R$)")
plt.savefig("output/media_bens_eleitos.png")
plt.show()

import seaborn as sns

# Gráfico de dispersão para coligações
plt.figure(figsize=(10, 6))
sns.scatterplot(data=coligacoes_detalhadas, x='NUMERO_PARTIDOS', y='NUM_ELEITOS', hue='NM_COLIGACAO', palette="viridis")
plt.title("Número de Eleitos por Coligação x Número de Partidos")
plt.xlabel("Número de Partidos")
plt.ylabel("Número de Eleitos")
plt.legend([],[], frameon=False)
plt.savefig("output/coligacoes_eleitos.png")
plt.show()

fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# Indígenas por região
sns.barplot(data=indigenas_por_regiao, x='REGIAO', y='NUM_INDIGENAS', ax=ax[0], palette="Blues")
ax[0].set_title("Número de Candidatos Indígenas por Região")

# Quilombolas por região
sns.barplot(data=quilombolas_por_regiao, x='REGIAO', y='NUM_QUILOMBOLAS', ax=ax[1], palette="Greens")
ax[1].set_title("Número de Candidatos Quilombolas por Região")

plt.tight_layout()
plt.savefig("output/indigenas_quilombolas_regiao.png")
plt.show()

# scripts/análise_eleitoral.py

plt.figure(figsize=(10, 8))
sns.barplot(data=maior_partido_por_uf, y='SG_UF', x='NUM_CANDIDATOS', hue='SG_PARTIDO', dodge=False)
plt.title("Partido com Maior Quantidade de Candidatos por UF")
plt.xlabel("Número de Candidatos")
plt.ylabel("UF")
plt.legend(title="Partido")
plt.savefig("output/partido_maior_por_uf.png")
plt.show()

# scripts/análise_eleitoral.py

plt.figure(figsize=(12, 6))
sns.barplot(data=candidatos_por_regiao, x='REGIAO', y='NUM_CANDIDATOS', hue='SG_PARTIDO')
plt.title("Distribuição de Candidaturas por Partido e Região")
plt.xlabel("Região")
plt.ylabel("Número de Candidatos")
plt.legend(title="Partido", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.savefig("output/distribuicao_partido_regiao.png")
plt.show()

# scripts/análise_eleitoral.py

plt.figure(figsize=(10, 8))
sns.barplot(data=partido_dominante_uf, y='SG_UF', x='TOTAL_CANDIDATOS', hue='SG_PARTIDO', dodge=False)
plt.title("Partido Dominante por UF (Prefeito, Vice e Vereadores)")
plt.xlabel("Total de Candidatos")
plt.ylabel("UF")
plt.legend(title="Partido")
plt.savefig("output/partido_dominante_por_uf.png")
plt.show()

# scripts/análise_eleitoral.py

plt.figure(figsize=(12, 6))
sns.countplot(data=dados_redes_sociais, x='SG_UF', hue='TIPO_REDE', order=sorted(dados_redes_sociais['SG_UF'].unique()))
plt.title("Rede Social Preferida dos Candidatos por UF")
plt.xlabel("UF")
plt.ylabel("Número de Candidatos")
plt.legend(title="Rede Social", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.savefig("output/rede_social_uf.png")
plt.show()

# scripts/análise_eleitoral.py

from wordcloud import WordCloud

# Gerar nuvem de palavras para as propostas
text = ' '.join([term for term, _ in termos_frequentes])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Exibir a nuvem de palavras
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Principais Termos nas Propostas de Governo")
plt.savefig("output/nuvem_termos_propostas.png")
plt.show()
